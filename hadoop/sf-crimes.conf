#gbtree или gblinear
booster = gbtree
# choose logistic regression loss function for binary classification
objective = binary:logistic

# Tree Booster Parameters
# step size shrinkage
eta = 0.73
# minimum loss reduction required to make a further partition
gamma = 0.7
# minimum sum of instance weight(hessian) needed in a child
min_child_weight = 1
# maximum depth of a tree
max_depth = 300

# Task Parameters
# the number of round to do boosting
num_round = 500
# 0 means do not save any model except the final round model
save_period = 0
# evaluate on training data as well each round
# eval_train = 1
# The path of validation data, used to monitor training process, here [test] sets name of the validation set
# eval[test] = "clean_train.txt.test"

#data = hdfs:///data/
# The path of model file
#model_out =
# split pattern of xgboost
dsplit = row
# evaluate on training data as well each round
eval_train = 1